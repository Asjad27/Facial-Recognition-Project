{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"import sys, os\nimport pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nimport h5py\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.metrics import classification_report\nimport seaborn as sb\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading dataset and removing emotions","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/ck-plus-shuffled/ck_plus.csv\")\n\ndata = data[data['emotion'] != 1]\ndata = data[data['emotion'] != 2]\n\ndata.loc[data.emotion == 3, 'emotion'] = 1\ndata.loc[data.emotion == 4, 'emotion'] = 2\ndata.loc[data.emotion == 5, 'emotion'] = 3\ndata.loc[data.emotion == 6, 'emotion'] = 4\ndata.loc[data.emotion == 7, 'emotion'] = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining values reused throughout the code","metadata":{}},{"cell_type":"code","source":"num_features = 64\nnum_labels = 6\nbatch_size = 64\nepochs = 100\nwidth, height = 48, 48","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting output path and name of model file","metadata":{}},{"cell_type":"code","source":"BASEPATH = '../output/kaggle/working'\nMODELPATH = 'model_ck_plus.h5'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preparing images for analysis (includes normalization)","metadata":{}},{"cell_type":"code","source":"pixels = data['pixels'].tolist()\nfaces = []\nfor pixel_sequence in pixels:\n    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n    face = np.asarray(face).reshape(width, height)\n    faces.append(face.astype('float32')/255.0)\n\nfaces = np.asarray(faces)\nfaces = np.expand_dims(faces, -1)\n\nemotions = pd.get_dummies(data['emotion']).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining labels which represent each emotion","metadata":{}},{"cell_type":"code","source":"labels = {0: 'anger', 1: 'fear', 2: 'happiness', 3: 'neutrality', 4: 'sadness', 5: 'surprise'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing sample images from the dataset","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 5, figsize=(25, 12))\naxs = axs.ravel()\nfor i in range(15):\n    axs[i].imshow(faces[i][:,:,0], cmap='gray')\n    axs[i].set_title(str(labels[np.argmax(emotions[i])]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting in to train, test, and validation datasets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(num_labels, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing model summary","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling the model and defining options for the model","metadata":{}},{"cell_type":"code","source":"model.compile(loss=categorical_crossentropy,\n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpointer = ModelCheckpoint(MODELPATH, monitor='val_loss', verbose=1, save_best_only=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensorboard = TensorBoard(log_dir='./logs')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(np.array(X_train), np.array(y_train),\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(np.array(X_test), np.array(y_test)),\n          shuffle=True, callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining function for plotting the accuracy and loss of the model over the epochs","metadata":{}},{"cell_type":"markdown","source":"#### Taken from https://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search","metadata":{}},{"cell_type":"code","source":"def plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    \n    print(len(loss_list) , len(val_loss_list) , len(acc_list) , len(val_acc_list))\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    \n    ## Loss\n    plt.figure(1)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    ## Accuracy\n    plt.figure(2)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calling the above defined plot_history function","metadata":{}},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the performance of the model","metadata":{}},{"cell_type":"code","source":"scores = model.evaluate(np.array(X_test), np.array(y_test), batch_size=batch_size)\nprint(\"Loss: \" + str(scores[0]))\nprint(\"Accuracy: \" + str(scores[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=[np.argmax(im) for im in model.predict(np.array(X_test))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emotion_labels = [np.argmax(i) for i in np.array(y_test)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting the confusion matrix","metadata":{}},{"cell_type":"code","source":"em_names = ['fear','surprise','sadness', 'neutrality', 'happiness', 'anger']\nfig, ax = plt.subplots(figsize=(8, 8))\nsb.heatmap(ax=ax, \n           data=pd.DataFrame(tf.math.confusion_matrix(emotion_labels, predictions, num_classes=6).numpy().astype(int), \n                     index=em_names, \n                     columns=em_names\n           ), \n           annot=True, \n           annot_kws={\"size\": 10},\n           fmt='g'\n          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Printing the classification report","metadata":{}},{"cell_type":"code","source":"Y_test = np.argmax(y_test, axis=1) # Convert one-hot to index\ny_pred = np.argmax(model.predict(X_test), axis=-1)\nprint(classification_report(Y_test, y_pred, target_names= em_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting sample images misclassified by the model","metadata":{}},{"cell_type":"code","source":"rows = 0\ncols = 0\ndisagree = []\nfor i in range(len(emotion_labels)):\n    if emotion_labels[i] != predictions[i]:\n        disagree.append(i)\n        if len(disagree) == 10:\n            break\n\nfig, axs = plt.subplots(2, 5, figsize=(20,9))\n\nfor i in range(10):\n    image=(np.array(list(X_test)[disagree[i]])/256)[:,:,0]\n    axs[cols, rows].imshow(image, cmap='gray')\n    axs[cols, rows].set_title('Predicted: ' + str(labels[predictions[disagree[i]]]) + '\\nGround Truth: ' + str(labels[emotion_labels[disagree[i]]]))\n    rows = rows+1\n    if rows == 5:\n        cols = cols + 1\n        rows = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating the performance of VGG16 on the same dataset","metadata":{}},{"cell_type":"markdown","source":"## Normalizing and stacking the grayscale images into 3 bands as VGG requires 3 band images","metadata":{}},{"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\n\nfaces = []\nfor pixel_sequence in pixels:\n    face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n    face = np.asarray(face).reshape(width, height)\n    face = cv2.merge((face,face,face))\n    faces.append(face.astype('float32')/255.0)\n\nfaces = np.asarray(faces)\nfaces = np.expand_dims(faces, -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting in to train, test, and validation datasets","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing the VGG16 model","metadata":{}},{"cell_type":"code","source":"model_base = VGG16(include_top=False,\n                      weights='imagenet',\n                      input_shape=(width, height, 3))\nfor layer in model_base.layers[:-2]:\n    layer.trainable = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining the model using the VGG16 as the base and adding the Softmax layer","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(model_base)\nmodel.add(Flatten())\nmodel.add(Dense(num_labels, activation='softmax'))\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compiling, training, and evaluating the VGG16 model","metadata":{}},{"cell_type":"code","source":"model.compile(loss=categorical_crossentropy,\n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n              metrics=['accuracy'])\nmodel.fit(np.array(X_train), np.array(y_train),\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(np.array(X_test), np.array(y_test)),\n          shuffle=True, callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])\n\nscores = model.evaluate(np.array(X_test), np.array(y_test), batch_size=batch_size)\nprint(\"Loss: \" + str(scores[0]))\nprint(\"Accuracy: \" + str(scores[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}